{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivotal Greenplum-Spark Connector\n",
    "## PySpark Example\n",
    "\n",
    "----\n",
    "\n",
    "Pivotal Greenplum-Spark Connector documentation (notes below extracted from Pivotal documentation):\n",
    "\n",
    "https://greenplum-spark.docs.pivotal.io/110/index.html\n",
    "\n",
    "----\n",
    "\n",
    "Steps to launching Jupyter Notebook with Greenplum-Spark connector available\n",
    "\n",
    "1. Download greenplum-spark connector from Pivotal network https://network.pivotal.io/products/pivotal-gpdb (version used for this example greenplum-spark_2.11-1.1.0.jar)\n",
    "\n",
    "2. Set environment variables - pyspark will launch Jupyter Notebook\n",
    "```bash\n",
    "# set environment variables\n",
    "export PYSPARK_DRIVER_PYTHON='ipython'\n",
    "export PYSPARK_DRIVER_PYTHON_OPTS='notebook --port=8888 --no-browser --ip=0.0.0.0 --notebook_dir=/notebooks'\n",
    "```\n",
    "3. Launch Jupyter Notebook\n",
    "```bash\n",
    "# Launch notebooks\n",
    "# Set jar to location of greenplum-spark connector jar\n",
    "pyspark --master spark://spark:7077 --jars=../spark-jars/greenplum-spark_2.11-1.1.0.jar\n",
    "```\n",
    "\n",
    "*Note - Wine data set used in example https://archive.ics.uci.edu/ml/datasets/wine*\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Wine data set for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load wine dataset for example\n",
    "import psycopg2\n",
    "\n",
    "connString = \"host='192.168.99.100' dbname='gpadmin' user='gpadmin' password='pivotal' port=5432\"\n",
    "conn = psycopg2.connect(connString)\n",
    "conn.autocommit = True\n",
    "cur = conn.cursor()\n",
    "\n",
    "# create external web table\n",
    "query = \"\"\"\n",
    "    DROP EXTERNAL TABLE IF EXISTS public.wine_external;\n",
    "    CREATE EXTERNAL WEB TABLE public.wine_external (\n",
    "         cultivars integer\n",
    "        ,alcohol float\n",
    "        ,malic_acid float\n",
    "        ,ash float\n",
    "        ,alcalinity_of_ash float\n",
    "        ,magnesium float\n",
    "        ,total_phenols float\n",
    "        ,flavanoids float\n",
    "        ,nonflavanoid_phenols float\n",
    "        ,proanthocyanins float\n",
    "        ,color_intensity float\n",
    "        ,hue float\n",
    "        ,od280_od315 float\n",
    "        ,proline integer\n",
    "    ) LOCATION ('http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data')\n",
    "    FORMAT 'CSV';\n",
    "    \n",
    "    DROP TABLE IF EXISTS public.wine;\n",
    "    CREATE TABLE public.wine AS \n",
    "    SELECT * \n",
    "    FROM public.wine_external\n",
    "    DISTRIBUTED BY (cultivars);\n",
    "    \n",
    "\"\"\"\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PySpark Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import pyspark              # http://spark.apache.org/docs/latest/api/python/\n",
    "#from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " spark = SparkSession.builder \\\n",
    "     .master(\"local\") \\\n",
    "     .appName(\"Word Count\") \\\n",
    "     .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "     .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the .load() operation does not initiate the movement of data from Greenplum Database to Spark. \n",
    "Spark employs lazy evaluation for transformations; it does not compute the results until the application \n",
    "performs an action on the DataFrame, such as displaying or filtering the data or counting the number of rows.\n",
    "\n",
    "https://greenplum-spark.docs.pivotal.io/110/read_from_gpdb.html\n",
    "\n",
    "Options\n",
    "* **url** format jdbc:postgresql://[hostname]:[port]/[database]\n",
    "* **dbtable** table must be in GPDB search_path and have a distribution column (can not be distributed randomly)\n",
    "* **partitionColumn** must be of type in [bigint, bigserial, integer, serial]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-23f4ccc37448>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-23f4ccc37448>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    spark.\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "spark.\n",
    "# create pointer to table 'pivotal.testing' in greenplum\n",
    "#sqlContext = SQLContext(sc)\n",
    "gpdf = sqlContext.read.format(\"io.pivotal.greenplum.spark.GreenplumRelationProvider\").options(\n",
    "    url=\"jdbc:postgresql://192.168.99.100:5432/gpadmin\",\n",
    "    user=\"gpadmin\",\n",
    "    password=\"pivotal\",\n",
    "    dbtable=\"wine\",\n",
    "    partitionColumn=\"cultivars\").load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-725c510b102d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m gpdf = pyspark.sql.read(\"io.pivotal.greenplum.spark.GreenplumRelationProvider\").options(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"jdbc:postgresql://192.168.99.100:5432/gpadmin\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0muser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpadmin\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pivotal\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdbtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"wine\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "gpdf = pyspark.sql.read(\"io.pivotal.greenplum.spark.GreenplumRelationProvider\").options(\n",
    "    url=\"jdbc:postgresql://192.168.99.100:5432/gpadmin\",\n",
    "    user=\"gpadmin\",\n",
    "    password=\"pivotal\",\n",
    "    dbtable=\"wine\",\n",
    "    partitionColumn=\"cultivars\").load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: By default, Spark recomputes a transformed DataFrame each time you run an action on it. \n",
    "If you have a large data set on which you want to perform multiple transformations, you may choose \n",
    "to keep the DataFrame in memory for performance reasons. You can use the DataSet.persist() method \n",
    "for this purpose. Keep in mind that there are memory implications to persisting large data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpdf.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check out data types of columns\n",
    "gpdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Column names \n",
    "gpdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# row count\n",
    "gpdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show first 5 rows\n",
    "gpdf.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summary stats\n",
    "# toPandas(): pySpark dataframe -> pandas dataframe\n",
    "gpdf.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select a subset of columns\n",
    "gpdf.select(gpdf.columns[0:2]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select first 5 columns, filter results to where cultivars = 1 and show top 5 when ranked by alcohol\n",
    "\n",
    "# select columns -> filter rows -> order results by\n",
    "gpdf.select(gpdf.columns[0:5]).filter(\"cultivars = 1\").orderBy(\"alcohol\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running Spark SQL query against DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare temp table view for running SQL queries\n",
    "gpdf.createGlobalTempView(\"wine\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select first 5 columns, filter results to where cultivars = 1 and show top 5 when ranked by alcohol\n",
    "\n",
    "# prepare query\n",
    "query = \"\"\"\n",
    "    SELECT {} \n",
    "    FROM global_temp.wine \n",
    "    WHERE cultivars = 1\n",
    "    ORDER BY alcohol\n",
    "\"\"\".format(','.join(gpdf.columns[0:5]))\n",
    "\n",
    "# run query\n",
    "spark.sql(query).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
